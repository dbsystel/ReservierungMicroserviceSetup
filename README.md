# MicroserviceReservierung
===============

> üí° Dieses Repository ist das Ergebnis einer Zusammenarbeit mit dem HPI im Rahmen einer Bachelorarbeit aus dem Jahre 2016. Ziel war es, eine prototypische Microservicearchitektur zu entwickeln. **Das Projekt wurde planm√§√üig nach Abschluss der Arbeit beendet.**

Dies ist ein gemeinsames Projekt zwischen DB Systel und Hasso-Plattner-Institut zur Implementierung eines Protoypen f√ºr die Sitzplatzreservierung auf Basis von Microservices

Anleitung zum Aufsetzen auf mehreren Amazon Web Services-Instanzen
---------------
1. Mehrere Instanzen aufsetzen 
2. Eine Instanz als Basis vor Etcd und Consul w√§hlen (--> IP Adresse merken < backend-ip >): Consul und Etcd sind das Backend f√ºr die Erstellung von Docker Swarm und ein Overlay-Netzwerk

3. Auf jeder Instanz ausf√ºhren: ./init.sh < backend-ip >

4. Auf der Backend-Instanz: ./etcd.sh 

5. Auf der Backend-Instanz: ./consul_setup.sh

6. Auf den Verwalter-Instanzen: ./swarm_manager.sh < backend-ip >

7. Auf den Knoten-Instanzen: ./swarm_node.sh < backend-ip >

8. Auf einer Verwalter-Instanz: ./setup < Anzahl der Knoten Instanzen >

Die graphische Oberfl√§che ist unter den IP-Adressen der Knoten-Instanzen unter Port 3000 erreichbar.

Die RabbitMQ-Management-Oberfl√§che befindet auch auf einer der Knoten-Instanzenauf Port 15672.

Auf einem beliebigen Verwalterknoten kann der Zustand des Clusters √ºberpr√ºft werden. Alle Dienstknoten m√ºssen angezeigt werden. Dazu wird der Befehl docker -H :4000 info verwendet. 

Erkl√§rung
---------------
<b>init.sh</b> 

Das Skript init.sh l√∂scht den bisheringen Key, den der Docker Deamon besitzt. Da die Instanzen alle auf demselben Snapshot beruhen k√∂nnen, ist dieser am Anfang f√ºr alle Instanzen gleich. Dadurch k√∂nnen die Knoten nicht voneinander unterschieden werdne. Indem man die Datei /etc/docker/key.json l√∂scht und Docker neustartet, wird ein neuer Key generiert. Au√üerdem wird der Docker Deamon entsprechend der Nutzung f√ºr Overlay-Netzewrk und Docker Swarm konfiguriert: Dazu muss er dir Flags  --cluster-advertise --cluster-store  erhalten, in dem man in die Datei /etc/default/docker folgende Zeile eingetr√§gt: 

<i>DOCKER_OPTS="-H tcp://0.0.0.0:2375 -H unix:///var/run/docker.sock --cluster-advertise eth0:2375 --cluster-store etcd://<etcd_ip>:2379" </i>
etcd.sh

Das Skript etcd.sh l√§d etcd herunter und legt es unter /usr/local/bin ab. Au√üerdem legt einen Hintergrundprozess an. √úber etcd k√∂nnen Overlay-Netzwerke erstellt werden.

<b>consul_setup.sh</b>

Durch das consul.sh Skript wird das Overlay-Netzwerk reservierung erstellt und au√üerdem ein Consul-Container hochgefahren, durch den der die Knoten einander sp√§ter finden und registrieren k√∂nnen.

<b>swarm_manager.sh</b>

swarm_manager.sh macht die Instanz zum Verwalter des Schwarms. Dazu muss bekannt sein, auf welcher Instanz sich consul und etcd befinden, daher wird diese IP-Adresse als Parameter √ºbergeben.

Durch den Befehl <i>ADVERTISE_ADDRESS=`ifconfig eth0 2>/dev/null|awk '/inet addr:/ {print $2}'|sed 's/addr://'`</i> wird innerhalb des Skriptes die IP-Adresse von Docker ausgelesen, sodass diese bei der Registrierung verwendet werden kann, ohne, dass sie explizit angegeben werden muss. Dadurch ist das Skript auf einer beliebigen Maschine ausf√ºhrbar. 

<b>swarm_node.sh</b>

swarm_node.sh macht die Instanz zum Dienstknoten. Auch daf√ºr m√ºssen die Adressen von consul und etcd bekannt sein.

Vor dem Starten des Containers werden alle bisherigen Container gestoppt und entfernt, sodass der Knoten durch dieses Skript auch zur√ºckgesetzt werden kann.

Die Adresse des Knotens wird ebenfalls automatisch durch <i>ADVERTISE_ADDRESS=`ifconfig eth0 2>/dev/null|awk '/inet addr:/ {print $2}'|sed 's/addr://'`</i> ausgelesen.

<b>setup.sh</b>

setup.sh f√§hrt anschlie√üend die Infrastruktur hoch und bef√ºllt sie mit Daten. Anschlie√üend werden die Dienste gestartet. Die Zahl, die dazu √ºbergeben wird, gibt an, wie viele Warteschlangen, Cassandra-Instanzen und Datenbank-Kopien, sowie wie viele Instanzen der einzelnen Dienste gestartet werden sollen.  
Der Ablauf ist dabei wie folgt:

1.Starte die Infrastruktur (infrastructure-installation/docker-compose)

2.Erstelle das Cassandra und RabbitMQ-Cluster in einer For-Schleife. 

Dies ist notwendig, da die einzelnen Knoten dieser Cluster eindeutige Hostnamen besitzen m√ºssen und daher nicht durch die Docker-Compose-Skalierung gestartet werden k√∂nnen. Damit die einzelnen Knoten von au√üen trotzdem auch unter demselben Hostnamen erreichbar sind, wird zus√§tzlich das Flag net-alias f√ºr alle Knoten gleich gew√§hlt.

3.Erstelle die Kopen der MySQL-Datenbank

4.Bef√ºlle die Infrastruktur mit den Testdaten (infrastructure-installation/testdata)

5.Starte die Dienste mit dem Skalierungsgrad (services-setup/docker-compose) 
